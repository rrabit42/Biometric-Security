{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "얼굴인식 진짜!",
      "provenance": [],
      "collapsed_sections": [
        "HcO-OJxb93tr",
        "j3a1ctAc-Hku",
        "_3JoTtWM-gmd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bffaL3xL9TcO",
        "outputId": "ac9cfdc5-d3fb-4f2a-e498-0402866d7884"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFjnFWw0-N7o"
      },
      "source": [
        "#필요한 라이브러리 불러오기\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as pylab\n",
        "from sklearn.metrics import pairwise_distances_argmin\n",
        "from skimage.io import imread\n",
        "from sklearn.utils import shuffle\n",
        "from skimage import img_as_float\n",
        "\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.utils import np_utils  # to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JFch0J19uUb"
      },
      "source": [
        "W = 144\n",
        "H = 144\n",
        "Nout = 351\n",
        "\n",
        "epochs=100\n",
        "batch_size=100"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC-aWZCoEGyC"
      },
      "source": [
        "# 탐지 결과를 위한 함수들"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sy-zVrA9y4Z"
      },
      "source": [
        "# 탐지 결과(reacll, precision, f1-score)를 위한 함수들\n",
        "from keras import backend as K\n",
        "\n",
        "def recall(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
        "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
        "\n",
        "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return recall\n",
        "\n",
        "\n",
        "def precision(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
        "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
        "\n",
        "    # Precision = (True Positive) / (True Positive + False Positive)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return precision\n",
        "\n",
        "\n",
        "def f1score(y_target, y_pred):\n",
        "    _recall = recall(y_target, y_pred)\n",
        "    _precision = precision(y_target, y_pred)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
        "    \n",
        "    # return a single tensor value\n",
        "    return _f1score"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bek0pi0kEJoE"
      },
      "source": [
        "# plot 그래프"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er1HmJxp9Xow"
      },
      "source": [
        "# 학습 결과 분석을 위한 그래프 구현\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_acc(history, title=None):\n",
        "  # summarize history for accuracy\n",
        "  if not isinstance(history, dict):\n",
        "    history = history.history\n",
        "  plt.plot(history['accuracy'])\n",
        "  plt.plot(history['val_accuracy'])\n",
        "  if title is not None:\n",
        "    plt.title(title)\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Training', 'Verification'], loc=0)   # 두 선의 이름(Train, Test) 표시\n",
        "\n",
        "def plot_loss(history, title=None):\n",
        "  # summarize history for loss\n",
        "  if not isinstance(history, dict):\n",
        "    history = history.history\n",
        "  plt.plot(history['loss'])             # 학습 데이터로 구한 손실값\n",
        "  plt.plot(history['val_loss'])         # 검증 데이터로 구한 손실값\n",
        "  if title is not None:\n",
        "    plt.title(title)\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Training', 'Verification'], loc=0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4NXkfYlELRJ"
      },
      "source": [
        "# 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhpfKkrE9o6U"
      },
      "source": [
        "train_dir = './drive/MyDrive/02_face_training'\n",
        "filenames = os.listdir(train_dir)\n",
        "\n",
        "faces = []\n",
        "labels = []\n",
        "\n",
        "for filename in filenames[:-1]:\n",
        "    label = filename[0:4]\n",
        "    image = cv2.imread(train_dir+'/'+filename, 0)\n",
        "    # image = np.array(image).reshape((-1))\n",
        "    image = cv2.resize(image, (W,H))\n",
        "    faces.append(image)\n",
        "    labels.append(int(label))\n",
        "\n",
        "faces = np.asarray(faces)\n",
        "# faces = faces.astype('float32')\n",
        "\n",
        "# 1~9의 숫자로 된 출력값을 이진수(0/1)로 표현되는 벡터로 바꿈 (더 효율적)\n",
        "# 원핫인코딩\n",
        "labels = np_utils.to_categorical(labels, num_classes = Nout)\n",
        "\n",
        "faces = faces.reshape(-1,W,H,1)\n",
        "faces = faces / np.max(faces) # 정규화: 0~255로 되어 있는 입력값을 0~1로 바꿔줌(성능 최적화를 위함)\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(faces, labels, test_size=0.2, random_state=13)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcO-OJxb93tr"
      },
      "source": [
        "# 모델 시도 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anx9X9P693VB",
        "outputId": "64c5d08e-4d24-45a2-d90a-921ff204663a"
      },
      "source": [
        "rate = 0.5\n",
        "\n",
        "model = models.Sequential([\n",
        "  layers.Conv2D(64, (3,3), activation='relu', padding=\"same\", input_shape=(W, H, 1)),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.MaxPooling2D(pool_size=(2,2)),\n",
        "  layers.Dropout(rate),\n",
        "\n",
        "  layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "  layers.Conv2D(128, (3,3), activation='relu'),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "  layers.Conv2D(128, (3,3), activation='relu'),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "  layers.Flatten(),\n",
        "  layers.Dropout(rate),\n",
        "  layers.Dense(512, activation='relu'),\n",
        "  layers.Dropout(rate),\n",
        "  layers.Dense(Nout, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 144, 144, 64)      640       \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 144, 144, 64)      256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 72, 72, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 72, 72, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 70, 70, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 70, 70, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 33, 33, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 33, 33, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 14, 14, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 512)               3211776   \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 351)               180063    \n",
            "=================================================================\n",
            "Total params: 3,652,383\n",
            "Trainable params: 3,651,615\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcMgxOBT9-t1"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy', precision, recall, f1score])"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFercZZ295sL"
      },
      "source": [
        "# 모델 시도 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJVkon-a9vPg",
        "outputId": "076262ca-6f19-4ec9-ea60-d5c63a3e4006"
      },
      "source": [
        "model = models.Sequential([\n",
        "  layers.Conv2D(64, (3,3), activation='relu', padding=\"same\", input_shape=(W, H, 1)),\n",
        "  layers.MaxPooling2D(pool_size=(2,2)),\n",
        "  layers.Dropout(0.25),\n",
        "\n",
        "  layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  layers.MaxPooling2D(pool_size=(2,2)),\n",
        "  layers.Dropout(0.25),\n",
        "\n",
        "  layers.Conv2D(128, (3,3), activation='relu'),\n",
        "  layers.MaxPooling2D(pool_size=(2,2)),\n",
        "  layers.Dropout(0.25),\n",
        "\n",
        "  layers.Conv2D(128, (3,3), activation='relu'),\n",
        "  layers.MaxPooling2D(pool_size=(2,2)),\n",
        "  layers.Dropout(0.25),\n",
        "\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(512, activation='relu'),\n",
        "  layers.Dropout(0.5),\n",
        "  layers.Dense(Nout, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 144, 144, 64)      640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 72, 72, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 72, 72, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 70, 70, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 33, 33, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               3211776   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 351)               180063    \n",
            "=================================================================\n",
            "Total params: 3,650,847\n",
            "Trainable params: 3,650,847\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19K1Y1zN-Fte"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy', precision, recall, f1score])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3a1ctAc-Hku"
      },
      "source": [
        "# 모델 시도3(Resnet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdXSSLLS-E0G",
        "outputId": "072788b6-d026-4561-d0cf-950a20044047"
      },
      "source": [
        "# 모델 출처: https://rarena.tistory.com/entry/keras-%ED%8A%B9%EC%A0%95-%EB%AA%A8%EB%8D%B8%EB%A1%9C%EB%93%9C%ED%95%98%EC%97%AC-%EB%82%B4-%EB%A0%88%EC%9D%B4%EC%96%B4\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "\n",
        "W = 244\n",
        "H = 244\n",
        "\n",
        "input = layers.Input(shape=(W,H,1))\n",
        "model = ResNet50(input_tensor=input, include_top=False, weights=None, pooling='max')\n",
        "\n",
        "x = model.output\n",
        "x = layers.Dense(512)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation('relu')(x)\n",
        "x = layers.Dense(Nout, activation='softmax', name='softmax')(x)\n",
        "model = models.Model(model.input, x)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 244, 244, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 250, 250, 1)  0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 122, 122, 64) 3200        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 122, 122, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 122, 122, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 124, 124, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 61, 61, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 61, 61, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 61, 61, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 61, 61, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 61, 61, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 61, 61, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 61, 61, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 61, 61, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 61, 61, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 61, 61, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 61, 61, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 61, 61, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 61, 61, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 61, 61, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 61, 61, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 61, 61, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 61, 61, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 61, 61, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 61, 61, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 61, 61, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 61, 61, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 61, 61, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 61, 61, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 61, 61, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 61, 61, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 61, 61, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 61, 61, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 61, 61, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 61, 61, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 61, 61, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 61, 61, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 61, 61, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 61, 61, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 31, 31, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 31, 31, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 31, 31, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 31, 31, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 31, 31, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 31, 31, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 31, 31, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 31, 31, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 31, 31, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 31, 31, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 31, 31, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 31, 31, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 31, 31, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 31, 31, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 31, 31, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 31, 31, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 31, 31, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 31, 31, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 31, 31, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 31, 31, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 31, 31, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 31, 31, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 31, 31, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 31, 31, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 31, 31, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 31, 31, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 31, 31, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 31, 31, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 31, 31, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 31, 31, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 31, 31, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 31, 31, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 31, 31, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 31, 31, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 31, 31, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 31, 31, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 31, 31, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 31, 31, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 31, 31, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 31, 31, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 31, 31, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 31, 31, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pool (GlobalMaxPooling2D)   (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 512)          1049088     max_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 512)          2048        dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 512)          0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Dense)                 (None, 351)          180063      activation_8[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 24,812,639\n",
            "Trainable params: 24,758,495\n",
            "Non-trainable params: 54,144\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNmv5Dhu-qpQ"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy', precision, recall, f1score])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3JoTtWM-gmd"
      },
      "source": [
        "# 모델 시도4(Resnet 직접 구현)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd48UERf-Exp"
      },
      "source": [
        "# 모델 출처: https://eremo2002.tistory.com/76\n",
        "\n",
        "from keras import models, layers\n",
        "from keras import Input\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers, initializers, regularizers, metrics\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import BatchNormalization, Conv2D, Activation, Dense, GlobalAveragePooling2D, MaxPooling2D, ZeroPadding2D, Add\n",
        " \n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "input_tensor = Input(shape=(W, H, 1), dtype='float32', name='input')\n",
        " \n",
        " \n",
        "def conv1_layer(x):    \n",
        "    x = ZeroPadding2D(padding=(3, 3))(x)\n",
        "    x = Conv2D(64, (7, 7), strides=(2, 2))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = ZeroPadding2D(padding=(1,1))(x)\n",
        " \n",
        "    return x   \n",
        " \n",
        "\n",
        "def conv2_layer(x):         \n",
        "    x = MaxPooling2D((3, 3), 2)(x)     \n",
        " \n",
        "    shortcut = x\n",
        " \n",
        "    for i in range(3):\n",
        "        if (i == 0):\n",
        "            x = Conv2D(64, (1, 1), strides=(1, 1), padding='valid')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        "            \n",
        "            x = Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        " \n",
        "            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\n",
        "            shortcut = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(shortcut)            \n",
        "            x = BatchNormalization()(x)\n",
        "            shortcut = BatchNormalization()(shortcut)\n",
        " \n",
        "            x = Add()([x, shortcut])\n",
        "            x = Activation('relu')(x)\n",
        "            \n",
        "            shortcut = x\n",
        " \n",
        "        else:\n",
        "            x = Conv2D(64, (1, 1), strides=(1, 1), padding='valid')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        "            \n",
        "            x = Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        " \n",
        "            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\n",
        "            x = BatchNormalization()(x)            \n",
        " \n",
        "            x = Add()([x, shortcut])   \n",
        "            x = Activation('relu')(x)  \n",
        " \n",
        "            shortcut = x        \n",
        "    \n",
        "    return x\n",
        " \n",
        " \n",
        " \n",
        "def conv3_layer(x):        \n",
        "    shortcut = x    \n",
        "    \n",
        "    for i in range(4):     \n",
        "        if(i == 0):            \n",
        "            x = Conv2D(128, (1, 1), strides=(2, 2), padding='valid')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)        \n",
        "            \n",
        "            x = Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)  \n",
        " \n",
        "            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\n",
        "            shortcut = Conv2D(512, (1, 1), strides=(2, 2), padding='valid')(shortcut)\n",
        "            x = BatchNormalization()(x)\n",
        "            shortcut = BatchNormalization()(shortcut)            \n",
        " \n",
        "            x = Add()([x, shortcut])    \n",
        "            x = Activation('relu')(x)    \n",
        " \n",
        "            shortcut = x              \n",
        "        \n",
        "        else:\n",
        "            x = Conv2D(128, (1, 1), strides=(1, 1), padding='valid')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        "            \n",
        "            x = Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        " \n",
        "            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\n",
        "            x = BatchNormalization()(x)            \n",
        " \n",
        "            x = Add()([x, shortcut])     \n",
        "            x = Activation('relu')(x)\n",
        " \n",
        "            shortcut = x      \n",
        "            \n",
        "    return x\n",
        " \n",
        " \n",
        " \n",
        "def conv4_layer(x):\n",
        "    shortcut = x        \n",
        "  \n",
        "    for i in range(6):     \n",
        "        if(i == 0):            \n",
        "            x = Conv2D(256, (1, 1), strides=(2, 2), padding='valid')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)        \n",
        "            \n",
        "            x = Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)  \n",
        " \n",
        "            x = Conv2D(1024, (1, 1), strides=(1, 1), padding='valid')(x)\n",
        "            shortcut = Conv2D(1024, (1, 1), strides=(2, 2), padding='valid')(shortcut)\n",
        "            x = BatchNormalization()(x)\n",
        "            shortcut = BatchNormalization()(shortcut)\n",
        " \n",
        "            x = Add()([x, shortcut]) \n",
        "            x = Activation('relu')(x)\n",
        " \n",
        "            shortcut = x               \n",
        "        \n",
        "        else:\n",
        "            x = Conv2D(256, (1, 1), strides=(1, 1), padding='valid')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        "            \n",
        "            x = Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        " \n",
        "            x = Conv2D(1024, (1, 1), strides=(1, 1), padding='valid')(x)\n",
        "            x = BatchNormalization()(x)            \n",
        " \n",
        "            x = Add()([x, shortcut])    \n",
        "            x = Activation('relu')(x)\n",
        " \n",
        "            shortcut = x      \n",
        " \n",
        "    return x\n",
        " \n",
        " \n",
        " \n",
        "def conv5_layer(x):\n",
        "    shortcut = x    \n",
        "  \n",
        "    for i in range(3):     \n",
        "        if(i == 0):            \n",
        "            x = Conv2D(512, (1, 1), strides=(2, 2), padding='valid')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)        \n",
        "            \n",
        "            x = Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)  \n",
        " \n",
        "            x = Conv2D(2048, (1, 1), strides=(1, 1), padding='valid')(x)\n",
        "            shortcut = Conv2D(2048, (1, 1), strides=(2, 2), padding='valid')(shortcut)\n",
        "            x = BatchNormalization()(x)\n",
        "            shortcut = BatchNormalization()(shortcut)            \n",
        " \n",
        "            x = Add()([x, shortcut])  \n",
        "            x = Activation('relu')(x)      \n",
        " \n",
        "            shortcut = x               \n",
        "        \n",
        "        else:\n",
        "            x = Conv2D(512, (1, 1), strides=(1, 1), padding='valid')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        "            \n",
        "            x = Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        " \n",
        "            x = Conv2D(2048, (1, 1), strides=(1, 1), padding='valid')(x)\n",
        "            x = BatchNormalization()(x)           \n",
        "            \n",
        "            x = Add()([x, shortcut]) \n",
        "            x = Activation('relu')(x)       \n",
        " \n",
        "            shortcut = x                  \n",
        " \n",
        "    return x\n",
        " \n",
        " \n",
        " \n",
        "x = conv1_layer(input_tensor)\n",
        "x = conv2_layer(x)\n",
        "x = conv3_layer(x)\n",
        "x = conv4_layer(x)\n",
        "x = conv5_layer(x)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "output_tensor = Dense(Nout, activation='softmax')(x)\n",
        "\n",
        "model = Model(input_tensor, output_tensor)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8rwxOr4-kBO"
      },
      "source": [
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsdOrxck-ngl"
      },
      "source": [
        "# Train 돌리기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GP9K2FwK2p5"
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# 조기 종료\n",
        "earlystop = EarlyStopping(patience=10)\n",
        "\n",
        "# 학습률 조정\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                            patience=2,verbose=1,\n",
        "                                            factor=0.5, min_lr=0.00001)\n",
        "\n",
        "# callback 설정\n",
        "callback = [earlystop, learning_rate_reduction]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTb9NdFn-EvX",
        "outputId": "6b88f1a2-158e-41df-e01d-71029cafa352"
      },
      "source": [
        "history = model.fit(\n",
        "  train_X, train_Y,\n",
        "  epochs=epochs,\n",
        "  batch_size=batch_size,\n",
        "  validation_data=(test_X, test_Y),\n",
        "  shuffle=True,\n",
        "  # callbacks=callback,\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "9/9 [==============================] - 37s 297ms/step - loss: 5.8966 - accuracy: 0.0052 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 5.8643 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 1s 111ms/step - loss: 5.8570 - accuracy: 0.0066 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 5.8794 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 1s 109ms/step - loss: 5.8446 - accuracy: 0.0030 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 5.8967 - val_accuracy: 0.0048 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 1s 110ms/step - loss: 5.8302 - accuracy: 0.0075 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 5.9168 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 1s 111ms/step - loss: 5.7873 - accuracy: 0.0063 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 5.8900 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 1s 111ms/step - loss: 5.7576 - accuracy: 0.0083 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 5.9115 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 1s 111ms/step - loss: 5.6636 - accuracy: 0.0082 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 5.8927 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 1s 110ms/step - loss: 5.5388 - accuracy: 0.0180 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 5.8976 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 1s 112ms/step - loss: 5.3049 - accuracy: 0.0226 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 5.8905 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 1s 111ms/step - loss: 4.9476 - accuracy: 0.0499 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 5.6933 - val_accuracy: 0.0048 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 1s 111ms/step - loss: 4.5950 - accuracy: 0.0722 - precision: 0.1235 - recall: 0.0026 - f1score: 0.0050 - val_loss: 5.3526 - val_accuracy: 0.0095 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 1s 110ms/step - loss: 3.9874 - accuracy: 0.1209 - precision: 0.5046 - recall: 0.0114 - f1score: 0.0219 - val_loss: 5.0133 - val_accuracy: 0.0429 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 1s 110ms/step - loss: 3.6177 - accuracy: 0.1943 - precision: 0.3403 - recall: 0.0182 - f1score: 0.0340 - val_loss: 4.5837 - val_accuracy: 0.1524 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 1s 111ms/step - loss: 3.1389 - accuracy: 0.2491 - precision: 0.5714 - recall: 0.0580 - f1score: 0.1042 - val_loss: 4.3533 - val_accuracy: 0.2190 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 1s 111ms/step - loss: 2.9305 - accuracy: 0.2954 - precision: 0.4916 - recall: 0.0911 - f1score: 0.1533 - val_loss: 3.9838 - val_accuracy: 0.2619 - val_precision: 0.3333 - val_recall: 0.0067 - val_f1score: 0.0131\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 1s 111ms/step - loss: 2.4735 - accuracy: 0.3835 - precision: 0.7179 - recall: 0.1447 - f1score: 0.2392 - val_loss: 3.6750 - val_accuracy: 0.3571 - val_precision: 0.6000 - val_recall: 0.0533 - val_f1score: 0.0979\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 1s 112ms/step - loss: 2.1072 - accuracy: 0.4385 - precision: 0.7691 - recall: 0.2508 - f1score: 0.3745 - val_loss: 3.2880 - val_accuracy: 0.4429 - val_precision: 0.9744 - val_recall: 0.1267 - val_f1score: 0.2234\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 1s 111ms/step - loss: 1.8358 - accuracy: 0.5030 - precision: 0.7965 - recall: 0.3370 - f1score: 0.4714 - val_loss: 3.0538 - val_accuracy: 0.4571 - val_precision: 0.9407 - val_recall: 0.1700 - val_f1score: 0.2837\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 1s 110ms/step - loss: 1.5940 - accuracy: 0.5638 - precision: 0.7571 - recall: 0.3850 - f1score: 0.5100 - val_loss: 2.8346 - val_accuracy: 0.5333 - val_precision: 0.9738 - val_recall: 0.2300 - val_f1score: 0.3712\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 1s 112ms/step - loss: 1.4798 - accuracy: 0.5914 - precision: 0.7844 - recall: 0.4403 - f1score: 0.5625 - val_loss: 2.6770 - val_accuracy: 0.5524 - val_precision: 0.9462 - val_recall: 0.2900 - val_f1score: 0.4382\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 1s 112ms/step - loss: 1.1862 - accuracy: 0.6810 - precision: 0.8359 - recall: 0.5340 - f1score: 0.6507 - val_loss: 2.5618 - val_accuracy: 0.5952 - val_precision: 0.9322 - val_recall: 0.4633 - val_f1score: 0.6189\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 1s 112ms/step - loss: 0.9923 - accuracy: 0.7074 - precision: 0.8395 - recall: 0.6286 - f1score: 0.7185 - val_loss: 2.5350 - val_accuracy: 0.6048 - val_precision: 0.8450 - val_recall: 0.4533 - val_f1score: 0.5882\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 1s 111ms/step - loss: 0.9949 - accuracy: 0.7031 - precision: 0.8396 - recall: 0.6112 - f1score: 0.7071 - val_loss: 2.3893 - val_accuracy: 0.6190 - val_precision: 0.8309 - val_recall: 0.4300 - val_f1score: 0.5665\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 1s 111ms/step - loss: 0.7800 - accuracy: 0.7702 - precision: 0.8819 - recall: 0.6758 - f1score: 0.7650 - val_loss: 2.3255 - val_accuracy: 0.6571 - val_precision: 0.9255 - val_recall: 0.5567 - val_f1score: 0.6924\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 1s 112ms/step - loss: 0.7507 - accuracy: 0.7791 - precision: 0.8664 - recall: 0.6967 - f1score: 0.7720 - val_loss: 2.2579 - val_accuracy: 0.6714 - val_precision: 0.9183 - val_recall: 0.5700 - val_f1score: 0.7013\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 1s 113ms/step - loss: 0.6542 - accuracy: 0.8045 - precision: 0.8918 - recall: 0.7302 - f1score: 0.8028 - val_loss: 2.3619 - val_accuracy: 0.6857 - val_precision: 0.8771 - val_recall: 0.6200 - val_f1score: 0.7243\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.5488 - accuracy: 0.8128 - precision: 0.8656 - recall: 0.7691 - f1score: 0.8142 - val_loss: 2.3611 - val_accuracy: 0.6857 - val_precision: 0.9312 - val_recall: 0.6067 - val_f1score: 0.7339\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 1s 112ms/step - loss: 0.5153 - accuracy: 0.8458 - precision: 0.9106 - recall: 0.7792 - f1score: 0.8397 - val_loss: 2.3872 - val_accuracy: 0.6762 - val_precision: 0.8482 - val_recall: 0.5933 - val_f1score: 0.6980\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 1s 112ms/step - loss: 0.4868 - accuracy: 0.8526 - precision: 0.9061 - recall: 0.8013 - f1score: 0.8503 - val_loss: 2.2860 - val_accuracy: 0.6905 - val_precision: 0.8954 - val_recall: 0.6267 - val_f1score: 0.7367\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.4522 - accuracy: 0.8636 - precision: 0.8991 - recall: 0.8150 - f1score: 0.8549 - val_loss: 2.1979 - val_accuracy: 0.7000 - val_precision: 0.8947 - val_recall: 0.6733 - val_f1score: 0.7677\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 1s 113ms/step - loss: 0.4146 - accuracy: 0.8634 - precision: 0.9213 - recall: 0.8227 - f1score: 0.8691 - val_loss: 2.1980 - val_accuracy: 0.7190 - val_precision: 0.8407 - val_recall: 0.6533 - val_f1score: 0.7352\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.4855 - accuracy: 0.8483 - precision: 0.8957 - recall: 0.8010 - f1score: 0.8456 - val_loss: 2.1290 - val_accuracy: 0.7190 - val_precision: 0.9210 - val_recall: 0.7367 - val_f1score: 0.8165\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.3434 - accuracy: 0.8967 - precision: 0.9331 - recall: 0.8526 - f1score: 0.8908 - val_loss: 2.1935 - val_accuracy: 0.7571 - val_precision: 0.8752 - val_recall: 0.7167 - val_f1score: 0.7861\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.3627 - accuracy: 0.8767 - precision: 0.9186 - recall: 0.8419 - f1score: 0.8784 - val_loss: 2.4116 - val_accuracy: 0.7429 - val_precision: 0.8632 - val_recall: 0.7133 - val_f1score: 0.7802\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 1s 113ms/step - loss: 0.3446 - accuracy: 0.8957 - precision: 0.9318 - recall: 0.8555 - f1score: 0.8918 - val_loss: 2.2750 - val_accuracy: 0.7571 - val_precision: 0.9044 - val_recall: 0.7567 - val_f1score: 0.8233\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 1s 113ms/step - loss: 0.3074 - accuracy: 0.8962 - precision: 0.9280 - recall: 0.8792 - f1score: 0.9029 - val_loss: 2.2995 - val_accuracy: 0.7714 - val_precision: 0.9208 - val_recall: 0.7333 - val_f1score: 0.8160\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 1s 113ms/step - loss: 0.2923 - accuracy: 0.9058 - precision: 0.9335 - recall: 0.8757 - f1score: 0.9035 - val_loss: 2.3207 - val_accuracy: 0.7476 - val_precision: 0.9058 - val_recall: 0.7333 - val_f1score: 0.8104\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.2875 - accuracy: 0.9071 - precision: 0.9222 - recall: 0.8752 - f1score: 0.8980 - val_loss: 2.2753 - val_accuracy: 0.7714 - val_precision: 0.9023 - val_recall: 0.8000 - val_f1score: 0.8459\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.2902 - accuracy: 0.8910 - precision: 0.9317 - recall: 0.8693 - f1score: 0.8993 - val_loss: 2.1964 - val_accuracy: 0.7810 - val_precision: 0.9127 - val_recall: 0.7833 - val_f1score: 0.8426\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.2112 - accuracy: 0.9328 - precision: 0.9554 - recall: 0.9130 - f1score: 0.9337 - val_loss: 2.4648 - val_accuracy: 0.7476 - val_precision: 0.8834 - val_recall: 0.7700 - val_f1score: 0.8226\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.2440 - accuracy: 0.9304 - precision: 0.9494 - recall: 0.9121 - f1score: 0.9302 - val_loss: 2.2465 - val_accuracy: 0.7524 - val_precision: 0.8897 - val_recall: 0.7700 - val_f1score: 0.8253\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.2400 - accuracy: 0.9208 - precision: 0.9538 - recall: 0.9065 - f1score: 0.9293 - val_loss: 2.3230 - val_accuracy: 0.7714 - val_precision: 0.8876 - val_recall: 0.7767 - val_f1score: 0.8283\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.2201 - accuracy: 0.9266 - precision: 0.9337 - recall: 0.9114 - f1score: 0.9224 - val_loss: 2.4305 - val_accuracy: 0.7619 - val_precision: 0.8749 - val_recall: 0.7433 - val_f1score: 0.8033\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 1s 113ms/step - loss: 0.2310 - accuracy: 0.9212 - precision: 0.9433 - recall: 0.9127 - f1score: 0.9277 - val_loss: 2.3162 - val_accuracy: 0.7714 - val_precision: 0.8892 - val_recall: 0.7333 - val_f1score: 0.8036\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.2456 - accuracy: 0.9192 - precision: 0.9326 - recall: 0.9104 - f1score: 0.9212 - val_loss: 2.3469 - val_accuracy: 0.7524 - val_precision: 0.8988 - val_recall: 0.7933 - val_f1score: 0.8404\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.2933 - accuracy: 0.9161 - precision: 0.9422 - recall: 0.8922 - f1score: 0.9164 - val_loss: 2.2488 - val_accuracy: 0.7190 - val_precision: 0.8511 - val_recall: 0.7067 - val_f1score: 0.7711\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.2741 - accuracy: 0.9165 - precision: 0.9361 - recall: 0.8852 - f1score: 0.9098 - val_loss: 2.3425 - val_accuracy: 0.7619 - val_precision: 0.9065 - val_recall: 0.7700 - val_f1score: 0.8322\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.2072 - accuracy: 0.9414 - precision: 0.9555 - recall: 0.9275 - f1score: 0.9412 - val_loss: 2.4020 - val_accuracy: 0.7619 - val_precision: 0.8833 - val_recall: 0.7833 - val_f1score: 0.8299\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.1419 - accuracy: 0.9649 - precision: 0.9689 - recall: 0.9508 - f1score: 0.9598 - val_loss: 2.4405 - val_accuracy: 0.7762 - val_precision: 0.8883 - val_recall: 0.8300 - val_f1score: 0.8575\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.1397 - accuracy: 0.9491 - precision: 0.9617 - recall: 0.9407 - f1score: 0.9510 - val_loss: 2.4907 - val_accuracy: 0.7571 - val_precision: 0.8953 - val_recall: 0.7833 - val_f1score: 0.8355\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.1932 - accuracy: 0.9395 - precision: 0.9510 - recall: 0.9220 - f1score: 0.9362 - val_loss: 2.5470 - val_accuracy: 0.7619 - val_precision: 0.8800 - val_recall: 0.8167 - val_f1score: 0.8463\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.1529 - accuracy: 0.9553 - precision: 0.9668 - recall: 0.9441 - f1score: 0.9553 - val_loss: 2.5197 - val_accuracy: 0.7762 - val_precision: 0.8954 - val_recall: 0.8333 - val_f1score: 0.8625\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.1506 - accuracy: 0.9422 - precision: 0.9500 - recall: 0.9328 - f1score: 0.9412 - val_loss: 2.6127 - val_accuracy: 0.7571 - val_precision: 0.8681 - val_recall: 0.8200 - val_f1score: 0.8429\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.1545 - accuracy: 0.9429 - precision: 0.9547 - recall: 0.9347 - f1score: 0.9446 - val_loss: 2.4036 - val_accuracy: 0.7762 - val_precision: 0.8914 - val_recall: 0.7933 - val_f1score: 0.8392\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.1904 - accuracy: 0.9296 - precision: 0.9489 - recall: 0.9181 - f1score: 0.9332 - val_loss: 2.2525 - val_accuracy: 0.7714 - val_precision: 0.8982 - val_recall: 0.8267 - val_f1score: 0.8597\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.1601 - accuracy: 0.9589 - precision: 0.9750 - recall: 0.9508 - f1score: 0.9627 - val_loss: 2.3257 - val_accuracy: 0.7810 - val_precision: 0.8859 - val_recall: 0.8267 - val_f1score: 0.8543\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.1872 - accuracy: 0.9321 - precision: 0.9471 - recall: 0.9234 - f1score: 0.9350 - val_loss: 2.4108 - val_accuracy: 0.7905 - val_precision: 0.8928 - val_recall: 0.8000 - val_f1score: 0.8437\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.1948 - accuracy: 0.9319 - precision: 0.9466 - recall: 0.9241 - f1score: 0.9352 - val_loss: 2.3653 - val_accuracy: 0.7762 - val_precision: 0.8606 - val_recall: 0.7900 - val_f1score: 0.8228\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.1591 - accuracy: 0.9543 - precision: 0.9660 - recall: 0.9400 - f1score: 0.9528 - val_loss: 2.4652 - val_accuracy: 0.7714 - val_precision: 0.8721 - val_recall: 0.7400 - val_f1score: 0.7998\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 1s 117ms/step - loss: 0.1551 - accuracy: 0.9495 - precision: 0.9648 - recall: 0.9377 - f1score: 0.9510 - val_loss: 2.2676 - val_accuracy: 0.7667 - val_precision: 0.8892 - val_recall: 0.7500 - val_f1score: 0.8131\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 1s 117ms/step - loss: 0.1097 - accuracy: 0.9622 - precision: 0.9691 - recall: 0.9526 - f1score: 0.9607 - val_loss: 2.3526 - val_accuracy: 0.7667 - val_precision: 0.8767 - val_recall: 0.8267 - val_f1score: 0.8501\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 1s 117ms/step - loss: 0.1216 - accuracy: 0.9615 - precision: 0.9701 - recall: 0.9555 - f1score: 0.9627 - val_loss: 2.4243 - val_accuracy: 0.7905 - val_precision: 0.8968 - val_recall: 0.8533 - val_f1score: 0.8739\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 1s 118ms/step - loss: 0.1126 - accuracy: 0.9623 - precision: 0.9731 - recall: 0.9561 - f1score: 0.9645 - val_loss: 2.5394 - val_accuracy: 0.7762 - val_precision: 0.8861 - val_recall: 0.8333 - val_f1score: 0.8584\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.0984 - accuracy: 0.9670 - precision: 0.9707 - recall: 0.9646 - f1score: 0.9676 - val_loss: 2.6661 - val_accuracy: 0.7714 - val_precision: 0.8697 - val_recall: 0.7900 - val_f1score: 0.8275\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.0852 - accuracy: 0.9702 - precision: 0.9746 - recall: 0.9667 - f1score: 0.9706 - val_loss: 2.6818 - val_accuracy: 0.7667 - val_precision: 0.8831 - val_recall: 0.8333 - val_f1score: 0.8569\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.0980 - accuracy: 0.9697 - precision: 0.9769 - recall: 0.9637 - f1score: 0.9702 - val_loss: 2.6258 - val_accuracy: 0.7810 - val_precision: 0.8798 - val_recall: 0.8333 - val_f1score: 0.8555\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 1s 117ms/step - loss: 0.0745 - accuracy: 0.9734 - precision: 0.9788 - recall: 0.9707 - f1score: 0.9747 - val_loss: 2.5660 - val_accuracy: 0.7667 - val_precision: 0.8672 - val_recall: 0.8267 - val_f1score: 0.8459\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.1035 - accuracy: 0.9632 - precision: 0.9662 - recall: 0.9616 - f1score: 0.9639 - val_loss: 2.4326 - val_accuracy: 0.7857 - val_precision: 0.8972 - val_recall: 0.8267 - val_f1score: 0.8595\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.1150 - accuracy: 0.9693 - precision: 0.9761 - recall: 0.9633 - f1score: 0.9696 - val_loss: 2.3612 - val_accuracy: 0.7762 - val_precision: 0.8901 - val_recall: 0.8400 - val_f1score: 0.8639\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.1055 - accuracy: 0.9626 - precision: 0.9668 - recall: 0.9538 - f1score: 0.9603 - val_loss: 2.5088 - val_accuracy: 0.7714 - val_precision: 0.8485 - val_recall: 0.8067 - val_f1score: 0.8266\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.0965 - accuracy: 0.9687 - precision: 0.9689 - recall: 0.9624 - f1score: 0.9656 - val_loss: 2.5105 - val_accuracy: 0.7762 - val_precision: 0.8850 - val_recall: 0.8300 - val_f1score: 0.8561\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.0978 - accuracy: 0.9716 - precision: 0.9800 - recall: 0.9579 - f1score: 0.9687 - val_loss: 2.6409 - val_accuracy: 0.7810 - val_precision: 0.8937 - val_recall: 0.8233 - val_f1score: 0.8561\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.0834 - accuracy: 0.9728 - precision: 0.9744 - recall: 0.9669 - f1score: 0.9706 - val_loss: 2.6766 - val_accuracy: 0.7524 - val_precision: 0.8755 - val_recall: 0.7933 - val_f1score: 0.8324\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.0901 - accuracy: 0.9771 - precision: 0.9822 - recall: 0.9691 - f1score: 0.9756 - val_loss: 2.7755 - val_accuracy: 0.7762 - val_precision: 0.8863 - val_recall: 0.8300 - val_f1score: 0.8562\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.1219 - accuracy: 0.9604 - precision: 0.9673 - recall: 0.9580 - f1score: 0.9626 - val_loss: 2.7813 - val_accuracy: 0.7714 - val_precision: 0.8782 - val_recall: 0.8333 - val_f1score: 0.8545\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.0805 - accuracy: 0.9722 - precision: 0.9796 - recall: 0.9697 - f1score: 0.9746 - val_loss: 2.7738 - val_accuracy: 0.7714 - val_precision: 0.8746 - val_recall: 0.8367 - val_f1score: 0.8549\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.1273 - accuracy: 0.9632 - precision: 0.9669 - recall: 0.9600 - f1score: 0.9634 - val_loss: 2.4848 - val_accuracy: 0.7810 - val_precision: 0.8836 - val_recall: 0.8367 - val_f1score: 0.8591\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.0902 - accuracy: 0.9704 - precision: 0.9784 - recall: 0.9694 - f1score: 0.9739 - val_loss: 2.5291 - val_accuracy: 0.7905 - val_precision: 0.8869 - val_recall: 0.8400 - val_f1score: 0.8624\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.1108 - accuracy: 0.9614 - precision: 0.9729 - recall: 0.9533 - f1score: 0.9630 - val_loss: 2.5813 - val_accuracy: 0.7952 - val_precision: 0.8920 - val_recall: 0.8467 - val_f1score: 0.8681\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 1s 117ms/step - loss: 0.0997 - accuracy: 0.9725 - precision: 0.9775 - recall: 0.9696 - f1score: 0.9735 - val_loss: 2.5869 - val_accuracy: 0.7952 - val_precision: 0.8952 - val_recall: 0.8500 - val_f1score: 0.8715\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.0977 - accuracy: 0.9718 - precision: 0.9744 - recall: 0.9688 - f1score: 0.9716 - val_loss: 2.5145 - val_accuracy: 0.7952 - val_precision: 0.8976 - val_recall: 0.8433 - val_f1score: 0.8690\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.0637 - accuracy: 0.9774 - precision: 0.9825 - recall: 0.9763 - f1score: 0.9794 - val_loss: 2.6241 - val_accuracy: 0.7905 - val_precision: 0.8914 - val_recall: 0.8433 - val_f1score: 0.8662\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.0792 - accuracy: 0.9741 - precision: 0.9762 - recall: 0.9615 - f1score: 0.9688 - val_loss: 2.7597 - val_accuracy: 0.7857 - val_precision: 0.8811 - val_recall: 0.8333 - val_f1score: 0.8560\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.0731 - accuracy: 0.9728 - precision: 0.9734 - recall: 0.9640 - f1score: 0.9687 - val_loss: 2.7903 - val_accuracy: 0.8000 - val_precision: 0.8497 - val_recall: 0.8167 - val_f1score: 0.8326\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 1s 113ms/step - loss: 0.0698 - accuracy: 0.9777 - precision: 0.9805 - recall: 0.9754 - f1score: 0.9779 - val_loss: 2.8540 - val_accuracy: 0.8048 - val_precision: 0.8814 - val_recall: 0.8567 - val_f1score: 0.8687\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.1372 - accuracy: 0.9583 - precision: 0.9629 - recall: 0.9549 - f1score: 0.9589 - val_loss: 2.8813 - val_accuracy: 0.7381 - val_precision: 0.8255 - val_recall: 0.7800 - val_f1score: 0.8014\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.1185 - accuracy: 0.9640 - precision: 0.9723 - recall: 0.9610 - f1score: 0.9666 - val_loss: 2.5905 - val_accuracy: 0.7857 - val_precision: 0.8815 - val_recall: 0.8400 - val_f1score: 0.8598\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.0678 - accuracy: 0.9709 - precision: 0.9787 - recall: 0.9695 - f1score: 0.9741 - val_loss: 2.5136 - val_accuracy: 0.7857 - val_precision: 0.8838 - val_recall: 0.8367 - val_f1score: 0.8590\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.0702 - accuracy: 0.9792 - precision: 0.9864 - recall: 0.9769 - f1score: 0.9816 - val_loss: 2.5537 - val_accuracy: 0.7857 - val_precision: 0.8843 - val_recall: 0.8400 - val_f1score: 0.8612\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 1s 113ms/step - loss: 0.0807 - accuracy: 0.9766 - precision: 0.9818 - recall: 0.9759 - f1score: 0.9788 - val_loss: 2.5967 - val_accuracy: 0.7905 - val_precision: 0.8784 - val_recall: 0.8533 - val_f1score: 0.8655\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.0473 - accuracy: 0.9886 - precision: 0.9901 - recall: 0.9827 - f1score: 0.9864 - val_loss: 2.7689 - val_accuracy: 0.7952 - val_precision: 0.8811 - val_recall: 0.8533 - val_f1score: 0.8668\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 1s 116ms/step - loss: 0.0957 - accuracy: 0.9775 - precision: 0.9782 - recall: 0.9769 - f1score: 0.9775 - val_loss: 2.8147 - val_accuracy: 0.7905 - val_precision: 0.8766 - val_recall: 0.8433 - val_f1score: 0.8593\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.0914 - accuracy: 0.9707 - precision: 0.9765 - recall: 0.9666 - f1score: 0.9715 - val_loss: 2.6599 - val_accuracy: 0.7810 - val_precision: 0.8709 - val_recall: 0.8467 - val_f1score: 0.8585\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.0717 - accuracy: 0.9776 - precision: 0.9807 - recall: 0.9725 - f1score: 0.9766 - val_loss: 2.6211 - val_accuracy: 0.7952 - val_precision: 0.8849 - val_recall: 0.8100 - val_f1score: 0.8457\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.0711 - accuracy: 0.9818 - precision: 0.9857 - recall: 0.9778 - f1score: 0.9817 - val_loss: 2.6193 - val_accuracy: 0.8000 - val_precision: 0.8995 - val_recall: 0.8567 - val_f1score: 0.8770\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.0458 - accuracy: 0.9896 - precision: 0.9947 - recall: 0.9855 - f1score: 0.9900 - val_loss: 2.7402 - val_accuracy: 0.8143 - val_precision: 0.8913 - val_recall: 0.8633 - val_f1score: 0.8770\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.0483 - accuracy: 0.9756 - precision: 0.9809 - recall: 0.9738 - f1score: 0.9773 - val_loss: 2.8191 - val_accuracy: 0.8000 - val_precision: 0.8919 - val_recall: 0.8167 - val_f1score: 0.8526\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.0520 - accuracy: 0.9810 - precision: 0.9833 - recall: 0.9809 - f1score: 0.9821 - val_loss: 2.9181 - val_accuracy: 0.8143 - val_precision: 0.8937 - val_recall: 0.8600 - val_f1score: 0.8763\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 1s 115ms/step - loss: 0.0611 - accuracy: 0.9799 - precision: 0.9804 - recall: 0.9781 - f1score: 0.9792 - val_loss: 2.8792 - val_accuracy: 0.7952 - val_precision: 0.8779 - val_recall: 0.8533 - val_f1score: 0.8653\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 1s 114ms/step - loss: 0.1057 - accuracy: 0.9723 - precision: 0.9807 - recall: 0.9726 - f1score: 0.9767 - val_loss: 2.8970 - val_accuracy: 0.7810 - val_precision: 0.8759 - val_recall: 0.8433 - val_f1score: 0.8591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPfssDKn-ydo"
      },
      "source": [
        "# Test 돌리기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_KDyOvy-Es8",
        "outputId": "5c9d25f9-5d39-4a2a-8be6-881eac08c33e"
      },
      "source": [
        "loss, acc, precision, recall, f1score = model.evaluate(test_X, test_Y)\n",
        "\n",
        "print(\"\\nLoss: {}\\nAcc: {}\\nPrecision: {}\\nRecall: {}\\nF1-score: {}\".format(loss, acc, precision, recall, f1score))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 1s 50ms/step - loss: 2.8970 - accuracy: 0.7810 - precision: 0.8341 - recall: 0.7867 - f1score: 0.8096\n",
            "\n",
            "Loss: 2.8970086574554443\n",
            "Acc: 0.7809523940086365\n",
            "Precision: 0.8341384530067444\n",
            "Recall: 0.7867063879966736\n",
            "F1-score: 0.8095779418945312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "GapYCQG5-EqX",
        "outputId": "07705ea2-2967-4700-c13a-f0e37ac2fa19"
      },
      "source": [
        "plot_loss(history)\n",
        "plt.show()\n",
        "plot_acc(history)\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m8obTXo_abF"
      },
      "source": [
        "# 교차 검증"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbBFKzalAQpv"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold, KFold"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTsAlfJ_AMKI"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLJtxk8fAOgZ",
        "outputId": "93ba8670-7632-46c2-c298-e9151d4c9f35"
      },
      "source": [
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# cross validation\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "\n",
        "for train, test in kfold.split(train_X, train_Y):\n",
        "\n",
        "  # Define the model architecture\n",
        "  model = model\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  history = model.fit(train_X[train], train_Y[train],\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs,\n",
        "                      verbose=0)\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(train_X[test], train_Y[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Score for fold 1: loss of 0.001174612669274211; accuracy of 100.0%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Score for fold 2: loss of 0.00613543251529336; accuracy of 100.0%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Score for fold 3: loss of 3.0409453756874427e-05; accuracy of 100.0%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Score for fold 4: loss of 0.002871729899197817; accuracy of 100.0%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Score for fold 5: loss of 0.008002894930541515; accuracy of 99.40119981765747%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.001174612669274211 - Accuracy: 100.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.00613543251529336 - Accuracy: 100.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 3.0409453756874427e-05 - Accuracy: 100.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.002871729899197817 - Accuracy: 100.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.008002894930541515 - Accuracy: 99.40119981765747%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 99.8802399635315 (+- 0.23952007293701172)\n",
            "> Loss: 0.0036430158936127553\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzxSUXyVANcF"
      },
      "source": [
        "## test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnkYEVIH_ZyQ",
        "outputId": "18d7c072-ce2a-42a0-d25f-ff0178f4302c"
      },
      "source": [
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# cross validation\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "\n",
        "for train, test in kfold.split(test_X, test_Y):\n",
        "\n",
        "  # Define the model architecture\n",
        "  model = model\n",
        "\n",
        "  # # Generate a print\n",
        "  # print('------------------------------------------------------------------------')\n",
        "  # print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # # Fit data to model\n",
        "  # history = model.fit(X_train[train], Y_train[train],\n",
        "  #                     batch_size=batch_size,\n",
        "  #                     epochs=epochs,\n",
        "  #                     verbose=0)\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(test_X[test], test_Y[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score for fold 1: loss of 4.415487766265869; accuracy of 80.95238208770752%\n",
            "Score for fold 2: loss of 4.874973773956299; accuracy of 73.8095223903656%\n",
            "Score for fold 3: loss of 1.8489844799041748; accuracy of 80.95238208770752%\n",
            "Score for fold 4: loss of 3.2976536750793457; accuracy of 80.95238208770752%\n",
            "Score for fold 5: loss of 1.7444301843643188; accuracy of 83.33333134651184%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 4.415487766265869 - Accuracy: 80.95238208770752%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 4.874973773956299 - Accuracy: 73.8095223903656%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 1.8489844799041748 - Accuracy: 80.95238208770752%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 3.2976536750793457 - Accuracy: 80.95238208770752%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 1.7444301843643188 - Accuracy: 83.33333134651184%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 80.0 (+- 3.2296812791351925)\n",
            "> Loss: 3.2363059759140014\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkkvSW0r_F9z"
      },
      "source": [
        "# 결과 제출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sillgepo-En3",
        "outputId": "2ceb4d71-f340-4a84-9c92-91c819b7d673"
      },
      "source": [
        "import csv\n",
        "\n",
        "test_dir = './drive/MyDrive/02_face_test'\n",
        "file_list = os.listdir(test_dir)\n",
        "\n",
        "result = []\n",
        "\n",
        "for file in file_list:\n",
        "    name = file.split('.')[0]\n",
        "    data = cv2.imread(test_dir+'/'+file, 0)\n",
        "    data = cv2.resize(data, (W,H))\n",
        "    data = np.asarray(data)\n",
        "    data = data.reshape(-1,W,H,1)\n",
        "    data = data / np.max(data)\n",
        "    y = model.predict_classes(data)\n",
        "    result.append([name, y])\n",
        "  \n",
        "\n",
        "with open('./drive/MyDrive/1871063_김서영_얼굴_1차_답안.csv', 'w', newline='') as f:\n",
        "  write = csv.writer(f)\n",
        "  write.writerow(['Image', 'Answer'])\n",
        "  write.writerows(result)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBgmS2yZSj7X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}